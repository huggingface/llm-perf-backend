[project]
name = "llm-perf-backend"
version = "0.1.0"
description = "Backend for https://huggingface.co/spaces/optimum/llm-perf-leaderboard"
authors = [
    { name = "baptiste", email = "baptiste.colle@huggingface.co" }
]
dependencies = [
    "typer",
    "python-dotenv",
    "ruff",
    "packaging",
    "einops",
    "scipy",
    "optimum",
    "codecarbon",
    "transformers",
    "huggingface_hub[hf_transfer]",
    # "optimum-benchmark @ git+https://github.com/huggingface/optimum-benchmark.git",
    "optimum-benchmark @ file:./optimum-benchmark",
]
readme = "README.md"
requires-python = ">= 3.8"

[project.optional-dependencies]
onnxruntime = [
    "optimum-benchmark[onnx] @ git+https://github.com/huggingface/optimum-benchmark.git",
    # "optimum-benchmark[onnx] @ file:./optimum-benchmark",
    "onnx",
    "onnxruntime"
]
openvino = [
    # "optimum-benchmark[openvino] @ git+https://github.com/huggingface/optimum-benchmark.git",
    "optimum-benchmark[openvino] @ file:./optimum-benchmark",
]

[project.scripts]
llm-perf = "src.cli:app"
 
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.rye]
managed = true
dev-dependencies = []

[tool.hatch.metadata]
allow-direct-references = true

[tool.hatch.build.targets.wheel]
packages = ["src"]

[tool.ruff]
line-length = 120
lint.ignore = ["C901", "E501"]
lint.select = ["C", "E", "F", "I", "W", "I001"]

[tool.ruff.format]
line-ending = "auto"
quote-style = "double"
indent-style = "space"
skip-magic-trailing-comma = false

[tool.rye.scripts]
style = { chain = ["check-fix", "format" ] }
quality = { chain = ["check-quality", "format-check"] }
"check-fix" = "ruff check --fix"
"format" = "ruff format"
"check-quality" = "ruff check ."
"format-check" = "ruff format --check ."
"install-onnx" = "rye sync --features onnx"

# Running containers
run_cpu_container = "docker run -it --rm --pid host --volume .:/llm-perf-backend --workdir /llm-perf-backend ghcr.io/huggingface/optimum-benchmark:latest-cpu"
run_cuda_container = "docker run -it --rm --pid host --gpus all --shm-size 64G --volume .:/llm-perf-backend --workdir /llm-perf-backend ghcr.io/huggingface/optimum-benchmark:latest-cuda"
run_rocm_container = "docker run -it --rm --shm-size 64G --device /dev/kfd --device /dev/dri --volume .:/llm-perf-backend --workdir /llm-perf-backend ghcr.io/huggingface/optimum-benchmark:latest-rocm"

# Running benchmarks
run_cpu_benchmark = ""
run_cuda_benchmark = "rye run --features onnx -- run_cuda_container python src/benchmark_runners/cuda/run_benchmark.py"
run_rocm_benchmark = "rye run --features onnx -- run_rocm_container python src/benchmark_runners/rocm/run_benchmark.py"
